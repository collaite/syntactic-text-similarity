{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08caddd0-f4dd-4a30-bd93-7511d2d32b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "import textdistance\n",
    "import json\n",
    "from preprocess import preprocess_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bb2798-df2d-4c89-8548-b713e60630c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = 'data/results/syntactic/results-syn.csv'\n",
    "hammingfile = 'data/results/syntactic/results-syn-hamming.csv'\n",
    "jaccardfile = 'data/results/syntactic/results-syn-jaccard.csv'\n",
    "levenfile = 'data/results/syntactic/results-syn-leven.csv'\n",
    "sorensenfile = 'data/results/syntactic/results-syn-sorensen.csv'\n",
    "jarofile = 'data/results/syntactic/results-syn-jaro.csv'\n",
    "tverskyfile = 'data/results/syntactic/results-syn-tversky.csv'\n",
    "overlapfile = 'data/results/syntactic/results-syn-overlap.csv'\n",
    "bagfile = 'data/results/syntactic/results-syn-bag.csv'\n",
    "needlefile = 'data/results/syntactic/results-syn-needle.csv'\n",
    "lcsstrfile = 'data/results/syntactic/results-syn-lcsstr.csv'\n",
    "\n",
    "w1 = 'data/raw/ms-aladin-witness1-plagiarism.txt' \n",
    "w2 = 'data/raw/ms-aladin-witness2-plagiarism.txt'\n",
    "w3 = 'data/raw/ts-aladin-witness1-plagiarism.txt'\n",
    "w4 = 'data/raw/ts-aladin-witness2-plagiarism.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b292887-d558-4e7f-8c08-76d4b531daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords-iso.json', encoding='utf-8') as f:\n",
    "    stopwords = json.load(f)['nl']\n",
    "    \n",
    "other_stopwords = ['zooals','zoo','den','ter','ten','k','uwe','laten','gaat', 't']\n",
    "stopwords.extend(other_stopwords)\n",
    "    \n",
    "with open(w1, 'r') as f:\n",
    "    w1_raw = f.read()\n",
    "    \n",
    "with open(w2, 'r') as f:\n",
    "    w2_raw = f.read()\n",
    "\n",
    "with open(w3, 'r') as f:\n",
    "    w3_raw = f.read()\n",
    "    \n",
    "with open(w4, 'r') as f:\n",
    "    w4_raw = f.read()\n",
    "\n",
    "w1_preprocessed = preprocess_c(w1_raw, [])\n",
    "w2_preprocessed = preprocess_c(w2_raw, [])\n",
    "w3_preprocessed = preprocess_c(w3_raw, [])\n",
    "w4_preprocessed = preprocess_c(w4_raw, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12f0e55-f425-45de-92bb-a8578ecea09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "\n",
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(list_of_elem)\n",
    "        \n",
    "def remove_stopwords(source, sw):\n",
    "    if isinstance(source, str):\n",
    "        return preprocess_c(source, sw)\n",
    "    if isinstance(source, list):\n",
    "        return [w for w in source if not w in sw]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715c69c8-1a29-4606-bc92-46b3dadcca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_row = ['source_token', 'source', 'target', 'context_size', 'source_context_words', 'source_context_words_nstopw', 'source_context_inline', 'source_context_inline_nstopw', 'source_context_span', 'target_context_words', 'target_context_words_nstopw', 'target_context_inline', 'target_context_inline_nstopw', 'target_context_span', 'similarity_metric', 'similarity_score']\n",
    "append_list_as_row(outputfile, header_row)\n",
    "append_list_as_row(hammingfile, header_row)\n",
    "append_list_as_row(levenfile, header_row)\n",
    "append_list_as_row(jarofile, header_row)\n",
    "append_list_as_row(jaccardfile, header_row)\n",
    "append_list_as_row(lcsstrfile, header_row)\n",
    "append_list_as_row(tverskyfile, header_row)\n",
    "append_list_as_row(overlapfile, header_row)\n",
    "append_list_as_row(bagfile, header_row)\n",
    "append_list_as_row(needlefile, header_row)\n",
    "append_list_as_row(sorensenfile, header_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0deca9d-2007-4d97-a1f6-20c7840790ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(list_to_check, item_to_find):\n",
    "    indices = []\n",
    "    for idx, value in enumerate(list_to_check):\n",
    "        if value == item_to_find:\n",
    "            indices.append(idx)\n",
    "    return indices\n",
    "\n",
    "def find_context(search_token, list_of_tokens, context_size=1):\n",
    "    result = []\n",
    "    matching_indices = find_indices(list_of_tokens, search_token)\n",
    "    for matching_index in matching_indices:\n",
    "        curr_match = {}\n",
    "        curr_match['index'] = matching_index\n",
    "        curr_match['span'] = [matching_index, len(search_token)]\n",
    "        if ((matching_index-context_size >= 0) and (matching_index+context_size+1 <= len(list_of_tokens)-1)):\n",
    "            context = list_of_tokens[matching_index-context_size: matching_index+context_size+1]\n",
    "        elif ((matching_index-context_size < 0) and (matching_index+context_size+1 <= len(list_of_tokens)-1)):\n",
    "            context = list_of_tokens[: matching_index+context_size+1]\n",
    "        elif ((matching_index-context_size >= 0) and (matching_index+context_size+1 > len(list_of_tokens)-1)):\n",
    "            context = list_of_tokens[matching_index-context_size:]\n",
    "        else: # both (not possible)\n",
    "            return []\n",
    "        \n",
    "        inline = ' '.join(context)\n",
    "        curr_match['inline'] = inline\n",
    "        ind = context.index(search_token)\n",
    "        context.pop(ind)\n",
    "        words = context\n",
    "        curr_match['words'] = words\n",
    "        result.append(curr_match)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9306f463-2bdc-4b02-960c-859b2a8618e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preprocessed_sentences)\n",
    "# print()\n",
    "tokens_already_processed = []\n",
    "import needle\n",
    "\n",
    "def analyse_token_context(source, targets, context_size_range=[1,5]):\n",
    "    global tokens_already_processed\n",
    "    \n",
    "    results = []\n",
    "    for token in source['data']:\n",
    "        # print(token)\n",
    "        if ((token not in tokens_already_processed) and (token not in stopwords)): # do not process a token you already have, do not process a stopword token either\n",
    "            tokens_already_processed.append(token)\n",
    "            for i in range(context_size_range[0], context_size_range[1]+1):\n",
    "                source_contexts = find_context(token, list_of_tokens=source['data'], context_size=i)\n",
    "                # print(len(source_contexts))\n",
    "                for source_context in source_contexts:\n",
    "                    for target in targets:\n",
    "                        target_contexts = find_context(token, list_of_tokens=target['data'], context_size=i)\n",
    "                        \n",
    "                        if ((len(target_contexts) >= 1) and len(target_contexts[0]['inline']) > 0):\n",
    "                            for target_context in target_contexts:\n",
    "                                \n",
    "                                # Edit-distance based measures use inline text comparison\n",
    "                                metric = 'hamming'\n",
    "                                score = textdistance.hamming.normalized_similarity(remove_stopwords(source_context['inline'], stopwords), remove_stopwords(target_context['inline'], stopwords)) # hamming \n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'levenshtein'\n",
    "                                score = textdistance.levenshtein.normalized_similarity(remove_stopwords(source_context['inline'], stopwords), remove_stopwords(target_context['inline'], stopwords)) # levenshtein \n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'jaro_winkler'\n",
    "                                score = textdistance.jaro_winkler.normalized_similarity(remove_stopwords(source_context['inline'], stopwords), remove_stopwords(target_context['inline'], stopwords)) # jaro_winkler\n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'needleman_wunsch'\n",
    "                                needle_alignment = needle.NeedlemanWunsch(remove_stopwords(source_context['inline'], stopwords), remove_stopwords(target_context['inline'], stopwords))\n",
    "                                # needle_alignment.change_matrix(core.ScoreMatrix(1, -1, -1))\n",
    "                                needle_alignment.align()\n",
    "                                score = needle_alignment.get_score()\n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "\n",
    "                                # Token-based measures use words comparison\n",
    "                                metric = 'jaccard'\n",
    "                                score = textdistance.jaccard.normalized_similarity(remove_stopwords(source_context['words'], stopwords), remove_stopwords(target_context['words'], stopwords)) # jaccard\n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'sorensen'\n",
    "                                score = textdistance.sorensen.normalized_similarity(remove_stopwords(source_context['words'], stopwords), remove_stopwords(target_context['words'], stopwords)) # sorensen \n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'tversky'\n",
    "                                score = textdistance.tversky.normalized_similarity(remove_stopwords(source_context['words'], stopwords), remove_stopwords(target_context['words'], stopwords)) # tversky \n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])        \n",
    "                                metric = 'overlap'\n",
    "                                score = textdistance.overlap.normalized_similarity(remove_stopwords(source_context['words'], stopwords), remove_stopwords(target_context['words'], stopwords)) # overlap\n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "                                metric = 'bag'\n",
    "                                score = textdistance.bag.normalized_similarity(remove_stopwords(source_context['words'], stopwords), remove_stopwords(target_context['words'], stopwords)) # bag\n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])      \n",
    "\n",
    "                                # Sequence-based measures\n",
    "                                # metric = 'lcsseq'\n",
    "                                # score = textdistance.lcsseq.normalized_similarity(source_context['inline'], target_context['inline']) # lcsseq\n",
    "                                # results.append([token, source['name'], target['name'], i, source_context['words'], source_context['inline'], target_context['words'], target_context['inline'], metric, score])      \n",
    "                                metric = 'lcsstr'\n",
    "                                score = textdistance.lcsstr.normalized_similarity(remove_stopwords(source_context['inline'], stopwords), remove_stopwords(target_context['inline'], stopwords)) # lcsstr \n",
    "                                results.append([token, source['name'], target['name'], i, source_context['words'], remove_stopwords(source_context['words'], stopwords), source_context['inline'], remove_stopwords(source_context['inline'], stopwords), source_context['span'], target_context['words'], remove_stopwords(target_context['words'], stopwords), target_context['inline'], remove_stopwords(target_context['inline'], stopwords), target_context['span'], metric, score])                    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c376a0-8d93-4b57-8509-dd8ec0f721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_file(outputfile, results):\n",
    "    for item in results:\n",
    "        append_list_as_row(outputfile, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5263930-63e9-4284-9b1d-3e27e066b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: w1 - DONE\n",
      "FINISHED!\n"
     ]
    }
   ],
   "source": [
    "w1_results = analyse_token_context(source={'data' : w1_preprocessed, 'name' : 'w1'},targets=[{'data' : w2_preprocessed, 'name' : 'w2'},{'data' : w3_preprocessed, 'name' : 'w3'},{'data' : w4_preprocessed, 'name' : 'w4'}], context_size_range=[1,10])\n",
    "# write_results_to_file(outputfile, w1_results)\n",
    "print('source: w1 - DONE')\n",
    "# w2_results = analyse_token_context(source={'data' : w2_preprocessed, 'name' : 'w2'},targets=[{'data' : w1_preprocessed, 'name' : 'w1'},{'data' : w3_preprocessed, 'name' : 'w3'},{'data' : w4_preprocessed, 'name' : 'w4'}], context_size_range=[1,10])\n",
    "# write_results_to_file(outputfile, w2_results)\n",
    "# print('source: w2 - DONE')\n",
    "# w3_results = analyse_token_context(source={'data' : w3_preprocessed, 'name' : 'w3'},targets=[{'data' : w1_preprocessed, 'name' : 'w1'},{'data' : w2_preprocessed, 'name' : 'w2'},{'data' : w4_preprocessed, 'name' : 'w4'}], context_size_range=[1,10])\n",
    "# write_results_to_file(outputfile, w3_results)\n",
    "# print('source: w3 - DONE')\n",
    "# w4_results = analyse_token_context(source={'data' : w4_preprocessed, 'name' : 'w4'},targets=[{'data' : w1_preprocessed, 'name' : 'w1'},{'data' : w2_preprocessed, 'name' : 'w2'},{'data' : w3_preprocessed, 'name' : 'w3'}], context_size_range=[1,10])\n",
    "# write_results_to_file(outputfile, w4_results)\n",
    "# print('source: w4 - DONE')\n",
    "print('FINISHED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9fd3ca-734b-4ff9-8242-dff8b59aa456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd435207-388c-4012-ba9d-3d6619712605",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in w1_results:\n",
    "    if item[len(item)-2] == 'hamming':\n",
    "        write_results_to_file(hammingfile, [item])\n",
    "    if item[len(item)-2] == 'sorensen':\n",
    "        write_results_to_file(sorensenfile, [item])\n",
    "    if item[len(item)-2] == 'jaccard':\n",
    "        write_results_to_file(jaccardfile, [item])\n",
    "    if item[len(item)-2] == 'levenshtein':\n",
    "        write_results_to_file(levenfile, [item])\n",
    "    if item[len(item)-2] == 'jaro_winkler':\n",
    "        write_results_to_file(jarofile, [item])\n",
    "    if item[len(item)-2] == 'needleman_wunsch':\n",
    "        write_results_to_file(needlefile, [item])\n",
    "    if item[len(item)-2] == 'tversky':\n",
    "        write_results_to_file(tverskyfile, [item])\n",
    "    if item[len(item)-2] == 'overlap':\n",
    "        write_results_to_file(overlapfile, [item])\n",
    "    if item[len(item)-2] == 'bag':\n",
    "        write_results_to_file(bagfile, [item])\n",
    "    if item[len(item)-2] == 'lcsstr':\n",
    "        write_results_to_file(lcsstrfile, [item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c3b9b-31d3-42ca-8ff8-df2abb80e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(set(tokens_already_processed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6e9bb-3700-42c5-990b-14c427488f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52a6b1-4aa2-4ab5-bcc7-878fda353cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(textdistance.hamming.normalized_similarity('test', 'text')) # hamming "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
